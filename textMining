import warnings
import os
import snscrape.modules.twitter as sntwitter
import pandas as pd
import progressbar
from time import sleep

from wordcloud import WordCloud, STOPWORDS
from datetime import datetime, date

import stemming as stemming
import streamlit as st
from spacytextblob.spacytextblob import SpacyTextBlob
import re
import nltk
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer,PorterStemmer
from nltk.corpus import stopwords
from wordcloud import WordCloud
from better_profanity import profanity
import string
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import snscrape.modules.twitter as sntwitter
import pandas as pd

x= st.selectbox("Varlık Türü", ["Altın", "Petrol", "Dolar", "Euro","ASELSAN", "THY", "GARANTİ", "AKBANK", "BJK","Bitcoin", "Ethereum"])
current_time = datetime.now()
date = current_time.strftime("%Y-%m-%d")

# Creating list to append tweet data to
df = []

# Using TwitterSearchScraper to scrape data and append tweets to list
for i, tweet in enumerate(
        sntwitter.TwitterSearchScraper('x, today').get_items()):
    if i > 1000:
        break
    df.append([tweet.date, tweet.id, tweet.content])

# Creating a dataframe from the tweets list above
df = pd.DataFrame(df, columns=['Datetime', 'Tweet Id', 'Text'])
df.to_csv('x.csv')


import spacy
nlp = spacy.load('en_core_web_sm')
nlp.add_pipe("spacytextblob")

df['sentiment'] = df['Text'].apply(lambda x : nlp(x)._.polarity)
df_sentiment = df.sort_values('sentiment').reset_index(drop=True)


nltk.download('wordnet')
nltk.download('stopwords')

lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()
#adding a counter to check the progress of the algo while it runs
global counter
counter = 0

def preprocess(sentence, stemming=False, lemmatizing=False):
    global counter
    counter += 1
    if counter % 100 == 0:
        pass
#print(counter)
#clean as much as possible, but not apply strong editing to the text, yet
    sentence=str(sentence)
    tokenizer = RegexpTokenizer(r'\w+')
    sentence = sentence.lower()
    sentence=sentence.replace('{html}',"")
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, '', sentence)
    rem_url=re.sub(r'http\S+', '',cleantext)
    rem_num = re.sub('[0-9]+', '', rem_url)
    tokens = tokenizer.tokenize(rem_num)
    filtered_words = [w for w in tokens if len(w) > 2
    if not w in stopwords.words('english')]
    if stemming == True and lemmatizing == False:
        stem_words=[stemmer.stem(w) for w in filtered_words]
        return " ".join(stem_words)
    if stemming == False and lemmatizing == True:
        lemma_words=[lemmatizer.lemmatize(w) for w in filtered_words]
        return " ".join(lemma_words)
    if stemming == True and lemmatizing == True:
        stem_words=[stemmer.stem(w) for w in filtered_words]
        lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]
        return " ".join(lemma_words)
    #at the end of the algo we return filtered words
    return " ".join(filtered_words)

#preprocess the sentiment text
df_sentiment['Text'] = df_sentiment['Text'].apply(lambda x: preprocess(x, stemming=False, lemmatizing=True))


df_neg = df_sentiment[df_sentiment['sentiment'] < 0]
df_pos = df_sentiment[df_sentiment['sentiment'] > 0]

st.write("Negatif Tweet:", len(df_neg))
st.write("Pozitif Tweet:", len(df_pos))


#creating a string of positive tweets to analyze the words
positive_tweets = df[df['sentiment'] == 1]['Text'].tolist()
positive_tweets_string = " ".join(positive_tweets)
plt.figure(figsize=(10,10))
plt.imshow(WordCloud().generate(positive_tweets_string), interpolation = 'bilinear')
fig, ax = plt.subplots()

plt.show()
st.pyplot(fig)

#creating a string of negative tweets to analyze the words
negative_tweets = df[df['sentiment'] == 0]['Text'].tolist()
negative_tweets_string = " ".join(negative_tweets)
plt.figure(figsize=(10,10))
plt.imshow(WordCloud().generate(negative_tweets_string), interpolation = 'bilinear')
fig, ax = plt.subplots()

plt.show()
st.pyplot(fig)
